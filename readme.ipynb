{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "This is the second project of the Self-Driving Cars Nanodegree program from Udacity. The project notebook is in [project.ipynb](./project.ipynb) file.\n",
    "\n",
    "NOTE: **The source code of project is in the `./src/` folder** any other notebook in the repository it's used for test purposes only. You can use notebooks in the `./notebooks/` folder for a better comprehension of the process that i made.\n",
    "\n",
    "\n",
    "## Find Lane Process\n",
    "\n",
    "The steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "\n",
    "[original]: ./examples/original.jpg \"Original\"\n",
    "[undistorted]: ./examples/undistorted.jpg \"Undistorted\"\n",
    "[l_channel]: ./examples/l_channel.jpg \"L channel\"\n",
    "[s_channel]: ./examples/s_channel.jpg \"S Channel\"\n",
    "[sobel_binary]: ./examples/sobel_filter.jpg \"Sobel filter\"\n",
    "[mag_dir_binary]: ./examples/mag_dir_filter.jpg \"Magnitude+Direction filter\"\n",
    "[full_binary]: ./examples/full_binary.jpg \"Full binary\"\n",
    "[warp_area]: ./examples/warp_area.jpg \"Warp area\"\n",
    "[warped]: ./examples/warped.jpg \"Warped image\"\n",
    "[sliding_window]: ./examples/sliding_window.jpg \"Sliding window\"\n",
    "[around_window]: ./examples/search_around.jpg \"Search around\"\n",
    "[output]: ./examples/output.jpg \"Output\"\n",
    "[video1]: ./test_videos_output/project_video.mp4 \"Video\"\n",
    "\n",
    "### Original image\n",
    "![Original image][original]\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "The code of this part is in `./src/camera.py` in the `calibrate()` function.\n",
    "\n",
    "The code for this step is contained  cell of the IPython notebook located in \"./examples/example.ipynb\" (or in lines # through # of the file called `some_file.py`).  \n",
    "\n",
    "In order to calibrate the i need to collect object points in real world and image points that are the projection into the 2D image from the camera\n",
    "\n",
    "* Read a calibration images\n",
    "* Get chessboard points on each image and save it into the image points array\n",
    "* Calibrate camera\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "![Undistorted image][undistorted]\n",
    "\n",
    "### Color transform\n",
    "\n",
    "The code of this part is in `./src/gradient.py` in the `executePipeline()` function.\n",
    "\n",
    "In order to simplify the image to detect line lanes on the image I have to filter the image and reduce it to a binary image black/white.\n",
    "\n",
    "I choose the HLS color space and in particular the L and the S channels to extract the binary image.\n",
    "In this process I use the Sobel filter using `cv2.Soblel()` function, this will calculate a gradient value for each pixel that increase where pixels are parts of straight line, vertical or horizontal depends on the direction values paramters of the sobel function.\n",
    "\n",
    "#### S Channel\n",
    "\n",
    "![S Channel][s_channel]\n",
    "\n",
    "On the L channel I apply a  the following steps:\n",
    "* Apply Sobel filter on X direction\n",
    "* Filter the image and convert it to a binary image by apply a threshold\n",
    "\n",
    "* Apply Sobel filter on Y direction\n",
    "* Filter the image and convert it to a binary image by apply a threshold\n",
    "\n",
    "* Apply an `AND` operation between results binary images\n",
    "\n",
    "![Sobel binary][sobel_binary]\n",
    "\n",
    "#### L Channel\n",
    "\n",
    "![L Channel][l_channel]\n",
    "\n",
    "On the L channel I apply a  the following steps:\n",
    "* Apply sobel filter x and y directions \n",
    "* Calculate the magnitude of each pixel using the pythagorean theorem with the absolute gradients value from Sobels filters outputs (X and Y): \n",
    " \n",
    " $\\sqrt{(|sobel_x|)^2 + (|sobel_y|)^2}$\n",
    "\n",
    " \n",
    "* Filter the image and convert it to a binary image by apply a threshold\n",
    "\n",
    "* Calculate the direction of each pixel using the arctangent function with the absolute gradients value from Sobels filters outputs (X and Y):\n",
    "\n",
    " $arctan(|sobel_y|/|sobel_x|).$\n",
    " \n",
    "\n",
    "* Filter the image and convert it to a binary image by apply a threshold\n",
    "* Apply `AND` operation beetween Magnitude and Direction binary images\n",
    "\n",
    "It's a bit noisy but it's useful to get more details for the right lane.\n",
    "\n",
    "![Magnitude + Direction][mag_dir_binary]\n",
    "\n",
    "#### Binaries union\n",
    "\n",
    "In the and I apply an `OR` logic operation beetween the Result on S Channel and the result of the L channel\n",
    "\n",
    "![Full binary][full_binary]\n",
    "\n",
    "### Perspective Transform\n",
    "\n",
    "The code of this part is in `./src/camera.py` in the `birdsEyeTranform()` function and the transform coefficients are calculated into the constructor of the class.\n",
    "\n",
    "First I select 4 points on the original image to define a trapezoidal shape, I use an image where the road is straight as possibile. \n",
    "\n",
    "![Warp area][warp_area]\n",
    "\n",
    "The source and destination points are the following:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 591, 450      | 200, 0        | \n",
    "| 690, 450      | 1080, 0       |\n",
    "| 1122, 720     | 1080, 720     |\n",
    "| 191, 450      | 200, 720      |\n",
    "\n",
    "I put this source and destination points into the `cv2.getPerspectiveTransform(src, dst)` to get transform coefficients and change perspective of the image to a bird's eye view. \n",
    "\n",
    "![Warped image][warped]\n",
    "\n",
    "### Detect lane pixels\n",
    "\n",
    "The code of this part is in `./src/detector.py` in the `sliding_window()` function.\n",
    "\n",
    "The lane search starts with the binary image transformed into bird's eye view, on this image I start to find the lanes X position by extracting histogram based on the number of white pixels per column this give me the chance to find two high values on the image and get the X in the middle of each lane.\n",
    "\n",
    "For each value I define a rectangle where the width is 2x the `margin` value and the height is caluculated by a number of windows I'll create (`nwindows` property in the code). For each rectagle I select and store useful white pixels and for each rectangle I recalculate the mean on x positions of the pixels for the next. To avoid casual pixels cause a a large movement I recalculate the mean only if the number of pixels in the rectagle are grater than `minpix` value.\n",
    "\n",
    "##### Sliding window\n",
    "![Sliding window][sliding_window]\n",
    "\n",
    "The code of this part is in `./src/line.py` in the `get_line()` function.\n",
    "\n",
    "With all \"good pixels\" I can calculate a line that fit all pixels using the `polyfit` function of the numpy library.\n",
    "Note that the `polyfit` function returns the three coefficients of a second order polynomial for the following form:  $y = ax^2 + bx + c$ but I already have the Y values because are each pixel in vertical direction `np.linspace(0, height-1, height )` so I chose to invert X and Y pixels parameters of the `polyfit` functions.\n",
    "\n",
    "To avoid the slinding windows process that use a lot of time to execute in a video where lanes in frames are similar I use the line calculated in the first frame to find \"good pixels\".\n",
    "\n",
    "### Measure curvature \n",
    "\n",
    "Given the line coefficients I could calculate the the curvature of the line transformed to tbe real world. To do that I use this formula $\\frac{(1+(2*A*y + B)^2)^\\frac{3}{2}}{|2*B|}$ where `y` should be multiplied to a coefficient that in my case is $\\frac{44}{100}$. \n",
    "\n",
    "### Warp and draw lane boundaries onto the image\n",
    "\n",
    "When I have lane lines I can warp back and draw it onto the image.\n",
    "\n",
    "#### Lanes on original image\n",
    "![Output][output]\n",
    "\n",
    "\n",
    "### Project Video\n",
    "\n",
    "In the end I use the detector class to draw lane lines onto a video, frame by frame. To make more smooth the changes over frames I use the average coeffiecients between previous frames and the new one. \n",
    "Sometimes in a frame it's impossibile to find the lane due to an incorrect gradients filter, in this cases the `line` class returns the average of the previous lines. If I lost lanes for more than 5 frames I ask for a reset and the `detector` class redo a sliding window process.\n",
    "\n",
    "[Video](https://youtu.be/jgZo6PBEPls)\n",
    "\n",
    "\n",
    "## Conclusions and improvements\n",
    "\n",
    "The project took me a lot of time for trial and errors and some time for the knowledge that I had to refresh. But I think the code will work for simple videos. Here I want to explain some improvments that I choose to not implement for time. \n",
    "\n",
    "### Color channels\n",
    "\n",
    "I tried a lot of color spaces and operetiosn between them to reduce the noise of the image and increase edge detection. I think this is part could be majorly improved than others.\n",
    "- Using the Red channel of RGB it's possibile to get more details and well definde edges\n",
    "- Another improvments could be do some operation between H and S channels (in the HLS color space) because in the H channel the lanes are most of the time black.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
